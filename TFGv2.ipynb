{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_JNb5Ado4lS"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/victorinho09/Trabajo_Fin_Grado/blob/main/TFGv2.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORm3AGdSN9sT"
   },
   "source": [
    "First approach of a code base that loads a sample dataset and trains a basic neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3032,
     "status": "ok",
     "timestamp": 1740760664894,
     "user": {
      "displayName": "Victor",
      "userId": "05909548797419194511"
     },
     "user_tz": -60
    },
    "id": "K5GDpYD5P4DI",
    "outputId": "97a9b3be-43b7-443d-893b-dff7bffe0e21"
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import datetime\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49382,
     "status": "ok",
     "timestamp": 1740760714277,
     "user": {
      "displayName": "Victor",
      "userId": "05909548797419194511"
     },
     "user_tz": -60
    },
    "id": "sxDAjeQ9P-cv",
    "outputId": "9a631a14-3893-4053-d4ec-5f5dc353048c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/ucimlrepo/fetch.py:97: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_url)\n"
     ]
    }
   ],
   "source": [
    "data_iris = fetch_ucirepo(id=53).data #clasificacion\n",
    "data_heart_disease = fetch_ucirepo(id=45).data #clasificacion\n",
    "data_adult = fetch_ucirepo(id=2).data #clasificacion\n",
    "data_breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17).data #clasificacion\n",
    "data_bank_marketing = fetch_ucirepo(id=222).data #clasificacion\n",
    "data_diabetes = fetch_ucirepo(id=296).data #clasificacion\n",
    "data_mushroom = fetch_ucirepo(id=73).data #clasificacion\n",
    "data_default_payment = fetch_ucirepo(id=350).data #clasificacion\n",
    "data_car_evaluation = fetch_ucirepo(id=19).data #clasificacion\n",
    "data_dry_bean = fetch_ucirepo(id=602).data #clasificacion\n",
    "data_magic_gamma_telescope = fetch_ucirepo(id=159).data #clasificacion\n",
    "data_spambase = fetch_ucirepo(id=94).data #clasificacion\n",
    "data_census_income = fetch_ucirepo(id=20).data #clasificacion\n",
    "#data_phiusiil_phishing_url_website = fetch_ucirepo(id=967).data #clasificacion. --> NO CABE EN RAM, DA ERROR\n",
    "data_bike_sharing = fetch_ucirepo(id=275).data #regresion --> Este dataset funciona muy mal. ¿Mejorará con otra arquitectura?\n",
    "#data_real_estate_valuation = fetch_ucirepo(id=477).data #regresion --> Problema: Demasiado numero de clases objetivo, y muy pocas instancias, por lo que no da suficiente para que stratify funcione en testset\n",
    "data_communities_and_crime = fetch_ucirepo(id=183).data #regresion --> Funciona muy mal tmb, muchas clases objetivo\n",
    "#data_parkinsons_telemonitoring = fetch_ucirepo(id=189).data #regresion -->No coge datos, no contiene nada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740760714280,
     "user": {
      "displayName": "Victor",
      "userId": "05909548797419194511"
     },
     "user_tz": -60
    },
    "id": "lUfPdpdrbYVa"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_y_target_col(data_obj):\n",
    "    \"\"\"\n",
    "    Dado data_obj = fetch_ucirepo(id=...).data, localiza la columna\n",
    "    de 'targets' y la renombra a 'target'.\n",
    "    Maneja varios casos:\n",
    "     - Ya se llama 'target'\n",
    "     - Se llama 'class'\n",
    "     - Solo hay una columna en data.targets (la renombra a 'target')\n",
    "    Devuelve un DataFrame con una sola columna llamada 'target'.\n",
    "    \"\"\"\n",
    "    df_targets = data_obj.targets\n",
    "    cols = list(df_targets.columns)\n",
    "\n",
    "    # CASO 1: Si ya existe 'target'\n",
    "    if 'target' in cols:\n",
    "        y = df_targets[['target']].copy()\n",
    "        print(\"Numero cols en y:\", y.columns)\n",
    "        return y\n",
    "\n",
    "    # CASO 2: Si existe 'class'\n",
    "    if 'class' in cols:\n",
    "        y = df_targets[['class']].rename(columns={'class': 'target'})\n",
    "        print(\"Numero cols en y:\", y.columns)\n",
    "        return y\n",
    "\n",
    "    # CASO 3: Si solo hay 1 columna, la renombramos\n",
    "    if len(cols) == 1:\n",
    "        old_col = cols[0]\n",
    "        y = df_targets.rename(columns={old_col: 'target'})\n",
    "        return y[['target']]\n",
    "\n",
    "def preprocessData(data):\n",
    "\n",
    "    X = pd.DataFrame(data.features, columns=data.feature_names)\n",
    "    y = get_y_target_col(data)\n",
    "\n",
    "    #Elimino las filas que contienen que pertenecen a una clase con menos de 2 instancias en todo el dataset,\n",
    "    #porque sino stratify falla, al no poder balancear la aparición de clases en ambos sets al splittear\n",
    "    counts = y['target'].value_counts()\n",
    "    rare_classes = counts[counts < 2].index\n",
    "    mask = ~y['target'].isin(rare_classes)\n",
    "    X = X[mask].reset_index(drop=True)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "\n",
    "    #Filtrar filas donde y['target'] sea NaN\n",
    "    #(esto crea una máscara True/False, y solo conservas las True)\n",
    "    mask_not_nan = ~y['target'].isna()\n",
    "    X = X[mask_not_nan].reset_index(drop=True)\n",
    "    y = y[mask_not_nan].reset_index(drop=True)\n",
    "\n",
    "    #Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y #Hace falta stratify porque hay algún dataset (bike_sharing)\n",
    "        #que contiene muchas clases objetivo y provoca que no esté balanceado el train y el test set.\n",
    "    )\n",
    "\n",
    "    #print(y_train.info())\n",
    "    print(\"y_train count values:\", y_train['target'].nunique())\n",
    "    print(\"y_train values:\", y_train['target'].unique())\n",
    "    #print(y_train.head(30))\n",
    "\n",
    "    '''\n",
    "    print(\"X_train shape antes de imputer:\", X_train.shape)\n",
    "    print(\"¿NaNs en X_train?\", X_train.isna().sum())\n",
    "    '''\n",
    "\n",
    "    # 4) One-Hot Encoding para y\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_train_encoded = encoder.fit_transform(y_train['target'].values.reshape(-1, 1))\n",
    "    y_test_encoded  = encoder.transform(y_test['target'].values.reshape(-1, 1))\n",
    "    #print(\"y_test_encoded: \", y_test_encoded[1])\n",
    "    #print(\"size_y_test_encoded \", len(y_train_encoded[1]))\n",
    "\n",
    "    # 5) Separar columnas numéricas y categóricas\n",
    "    numeric_cols_train = X_train.select_dtypes(include=np.number).columns\n",
    "    categorical_cols_train = X_train.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "    if len(numeric_cols_train) == 0 and len(categorical_cols_train) == 0:\n",
    "      raise ValueError(\"No hay columnas numéricas ni categóricas en X_train\")\n",
    "\n",
    "    numeric_cols_test = X_test.select_dtypes(include=np.number).columns\n",
    "    categorical_cols_test = X_test.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "    if len(numeric_cols_test) == 0 and len(categorical_cols_test) == 0:\n",
    "      raise ValueError(\"No hay columnas numéricas ni categóricas en X_test\")\n",
    "\n",
    "    if(len(categorical_cols_train) == 0):\n",
    "        print(\"NO hay columnas categoricas en X_train\")\n",
    "\n",
    "    # Imputar numéricas\n",
    "    imputer_num = SimpleImputer(strategy='mean')\n",
    "    if len(numeric_cols_train) > 0:\n",
    "      X_train_num_imputed = imputer_num.fit_transform(X_train[numeric_cols_train])\n",
    "      X_test_num_imputed  = imputer_num.transform(X_test[numeric_cols_test])\n",
    "\n",
    "      # Escalar numéricas\n",
    "      scaler = StandardScaler()\n",
    "      X_train_num_scaled = scaler.fit_transform(X_train_num_imputed)\n",
    "      X_test_num_scaled  = scaler.transform(X_test_num_imputed)\n",
    "\n",
    "    else:\n",
    "      X_train_num_scaled = np.zeros((X_train.shape[0], 0))\n",
    "      X_test_num_scaled  = np.zeros((X_test.shape[0], 0))\n",
    "\n",
    "\n",
    "    # Imputar categóricas\n",
    "    imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "    if len(categorical_cols_train) > 0:\n",
    "      X_train_cat_imputed = imputer_cat.fit_transform(X_train[categorical_cols_train])\n",
    "      X_test_cat_imputed  = imputer_cat.transform(X_test[categorical_cols_test])\n",
    "    else:\n",
    "      X_train_cat_imputed = np.zeros((X_train.shape[0], 0))\n",
    "      X_test_cat_imputed  = np.zeros((X_test.shape[0], 0))\n",
    "\n",
    "    # Codificar categóricas\n",
    "\n",
    "    if(len(categorical_cols_train) != 0):\n",
    "        print(\"Hay columnas categoricas en X_train\")\n",
    "        ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        X_train_cat_encoded = ohe.fit_transform(X_train_cat_imputed)\n",
    "        X_test_cat_encoded  = ohe.transform(X_test_cat_imputed)\n",
    "\n",
    "    else:\n",
    "        X_train_cat_encoded = np.zeros((X_train.shape[0], 0))\n",
    "        X_test_cat_encoded  = np.zeros((X_test.shape[0], 0))\n",
    "\n",
    "    # Concatenar numéricas escaladas + categóricas codificadas\n",
    "    X_train_final = np.concatenate([X_train_num_scaled, X_train_cat_encoded], axis=1)\n",
    "    X_test_final  = np.concatenate([X_test_num_scaled, X_test_cat_encoded],  axis=1)\n",
    "\n",
    "    # Revisar si hay NaN en la salida final\n",
    "    print(\"¿Hay NaN en X_train_final?\", np.isnan(X_train_final).any())\n",
    "\n",
    "    return X_train_final, X_test_final, y_train_encoded, y_test_encoded\n",
    "\n",
    "#numero de neuronas rango (capa inicial) : raiz del numero de features - numero de features\n",
    "#numero capas:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1740760714282,
     "user": {
      "displayName": "Victor",
      "userId": "05909548797419194511"
     },
     "user_tz": -60
    },
    "id": "moHmIomtqnv_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import math\n",
    "\n",
    "# Creation of the ANN of example to try out the batch metrics in TensorBoard\n",
    "def create_model(X_train,y_train):\n",
    "  print(\"y_train_shape: \",y_train.shape[1])\n",
    "  model = Sequential([\n",
    "      Input(shape=(X_train.shape[1],)),\n",
    "      Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Hidden Layer 1\n",
    "      Dense(32, activation='relu'),  # Hidden Layer 2\n",
    "      Dense(y_train.shape[1], activation='softmax')  # Output Layer with softmax for multiclass clasification\n",
    "  ])\n",
    "\n",
    "  # Compiling the model\n",
    "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #uso categorical_crossentropy cuando las etiquetas están codificadas con one-hot encoder. Si no usaría: sparse_categ_cross\n",
    "\n",
    "  return model\n",
    "\n",
    "class GlobalBatchLogger(Callback):\n",
    "    def __init__(self, log_dir):\n",
    "        super(GlobalBatchLogger, self).__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.global_step = 0\n",
    "\n",
    "        # Variables para acumulados\n",
    "        self.cumulative_loss = 0.0\n",
    "        self.cumulative_accuracy = 0.0\n",
    "        self.total_samples = 0  # para saber cuántas muestras se han procesado\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # Creamos el escritor de resúmenes de TensorFlow al inicio del entrenamiento\n",
    "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
    "        self.writer.set_as_default()\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        \"\"\"\n",
    "        Se llama al terminar cada batch.\n",
    "        ‘batch’ es el índice local del batch en la época, pero usamos\n",
    "        self.global_step para numerar de forma continua a lo largo\n",
    "        de todo el entrenamiento.\n",
    "        \"\"\"\n",
    "        if logs is not None:\n",
    "            # Obtenemos las métricas *promediadas* que Keras ha calculado hasta este batch\n",
    "            batch_loss = logs.get('loss', 0.0)\n",
    "            batch_acc = logs.get('accuracy', 0.0)\n",
    "\n",
    "            # Determinamos cuántas muestras se incluyeron en este batch\n",
    "            batch_size = logs.get('size', None)\n",
    "            if batch_size is None:\n",
    "                # Si no aparece en logs, tomamos la configuración de la clase\n",
    "                batch_size = self.params.get('batch_size', 1)\n",
    "\n",
    "            # Actualizamos la suma acumulada de (loss * nº muestras) y (acc * nº muestras)\n",
    "            self.cumulative_loss += batch_loss * batch_size\n",
    "            self.cumulative_accuracy += batch_acc * batch_size\n",
    "            self.total_samples += batch_size\n",
    "\n",
    "            # Calculamos la media acumulada (hasta este batch)\n",
    "            avg_cumulative_loss = self.cumulative_loss / self.total_samples\n",
    "            avg_cumulative_accuracy = self.cumulative_accuracy / self.total_samples\n",
    "\n",
    "            # Registramos exclusivamente la métrica acumulada\n",
    "            tf.summary.scalar('cumulative_loss', data=avg_cumulative_loss, step=self.global_step)\n",
    "            tf.summary.scalar('cumulative_accuracy', data=avg_cumulative_accuracy, step=self.global_step)\n",
    "\n",
    "        # Forzamos la escritura en los ficheros de logs\n",
    "        self.writer.flush()\n",
    "\n",
    "        # Incrementamos el contador global de batches\n",
    "        self.global_step += 1\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        # Cerramos el writer al finalizar\n",
    "        self.writer.close()\n",
    "\n",
    "def compute_steps_for_batches(\n",
    "    desired_batches,\n",
    "    X_train_scaled,\n",
    "    batch_size=16\n",
    "):\n",
    "    \"\"\"\n",
    "    Dado un número deseado de batches (desired_batches), calcula\n",
    "    el número de steps que se pueden entrenar, sin pasarse\n",
    "    del total de batches en X_train_scaled.\n",
    "    - desired_batches: cuántos batches queremos.\n",
    "    - X_train_scaled: datos de entrenamiento.\n",
    "    - batch_size: tamaño de lote.\n",
    "\n",
    "    Retorna steps_for_n_batches, que es el mínimo entre desired_batches y\n",
    "    la cantidad real de batches que hay.\n",
    "    \"\"\"\n",
    "    total_samples = X_train_scaled.shape[0]\n",
    "    total_batches = math.ceil(total_samples / batch_size)\n",
    "\n",
    "    # Para no pasarnos de la época, usamos el mínimo\n",
    "    steps_for_n_batches = min(desired_batches, total_batches)\n",
    "\n",
    "    # También puedes forzar que sea al menos 1\n",
    "    if steps_for_n_batches < 1:\n",
    "        steps_for_n_batches = 1\n",
    "\n",
    "    return steps_for_n_batches\n",
    "\n",
    "def train_and_evaluate(dataset,num_batches,log_dir,batch_size):\n",
    "    X_train_scaled, X_test_scaled, y_train_encoded, y_test_encoded = preprocessData(dataset)\n",
    "    model = create_model(X_train_scaled,y_train_encoded)\n",
    "\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=1,\n",
    "        update_freq='epoch'\n",
    "    )\n",
    "    global_batch_logger = GlobalBatchLogger(log_dir)\n",
    "\n",
    "\n",
    "    steps_per_epoch = compute_steps_for_batches(num_batches,X_train_scaled,batch_size)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train_encoded,\n",
    "        validation_data=(X_test_scaled, y_test_encoded),\n",
    "        epochs=1,\n",
    "        steps_per_epoch = steps_per_epoch,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "        callbacks=[tb_callback,global_batch_logger]\n",
    "    )\n",
    "    loss, accuracy = model.evaluate(X_test_scaled, y_test_encoded)\n",
    "    print(f\"Steps_per_epoch: {steps_per_epoch},Pérdida: {loss:.4f}, Precisión: {accuracy:.4f}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5OTApYn9MI8"
   },
   "source": [
    "# **500 batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636
    },
    "executionInfo": {
     "elapsed": 5478,
     "status": "error",
     "timestamp": 1740760719761,
     "user": {
      "displayName": "Victor",
      "userId": "05909548797419194511"
     },
     "user_tz": -60
    },
    "id": "OHOanZui9F8c",
    "outputId": "8cdd3467-dfc5-48ce-8931-a664764aeaf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train count values: 94\n",
      "y_train values: [1.   0.01 0.56 0.11 0.06 0.36 0.18 0.44 0.05 0.1  0.04 0.22 0.15 0.03\n",
      " 0.02 0.16 0.07 0.57 0.2  0.09 0.14 0.25 0.48 0.41 0.08 0.29 0.19 0.12\n",
      " 0.54 0.   0.55 0.61 0.28 0.4  0.13 0.69 0.23 0.3  0.5  0.88 0.32 0.31\n",
      " 0.87 0.45 0.67 0.68 0.71 0.59 0.66 0.17 0.21 0.53 0.34 0.38 0.63 0.24\n",
      " 0.82 0.78 0.62 0.79 0.26 0.42 0.27 0.37 0.39 0.58 0.51 0.47 0.35 0.91\n",
      " 0.76 0.7  0.52 0.49 0.33 0.43 0.95 0.65 0.6  0.74 0.75 0.81 0.64 0.73\n",
      " 0.83 0.85 0.86 0.72 0.9  0.46 0.93 0.97 0.8  0.84]\n",
      "Hay columnas categoricas en X_train\n",
      "¿Hay NaN en X_train_final? False\n",
      "y_train_shape:  94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0779 - loss: 3.8177 \n",
      "Steps_per_epoch: 100,Pérdida: 3.8221, Precisión: 0.0704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16bee9640>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_and_evaluate(data_iris,500, \"logs/fit/iris/500batches\",16)\n",
    "train_and_evaluate(data_heart_disease,500, \"logs/fit/heart_disease/500batches\",16)\n",
    "train_and_evaluate(data_breast_cancer_wisconsin_diagnostic,500, \"logs/fit/breast_cancer_wisconsin_diagnostic/500batches\",16)\n",
    "train_and_evaluate(data_bank_marketing,500, \"logs/fit/bank_marketing/500batches\",16)\n",
    "train_and_evaluate(data_diabetes,500, \"logs/fit/diabetes/500batches\",16)\n",
    "train_and_evaluate(data_adult,500, \"logs/fit/adult/500batches\",16)\n",
    "train_and_evaluate(data_mushroom,500, \"logs/fit/mushroom/500batches\",16)\n",
    "train_and_evaluate(data_default_payment,500, \"logs/fit/default_payment/500batches\",16)\n",
    "train_and_evaluate(data_bike_sharing,500, \"logs/fit/bike_sharing/500batches\",16)\n",
    "train_and_evaluate(data_dry_bean,500, \"logs/fit/dry_bean/500batches\",16)\n",
    "train_and_evaluate(data_spambase,500, \"logs/fit/spambase/500batches\",16)\n",
    "train_and_evaluate(data_magic_gamma_telescope,500, \"logs/fit/magic_gamma_telescope/500batches\",16)\n",
    "#train_and_evaluate(data_real_estate_valuation,500, \"logs/fit/real_estate_valuation/500batches\",16)\n",
    "train_and_evaluate(data_car_evaluation,500, \"logs/fit/car_evaluation/500batches\",16)\n",
    "train_and_evaluate(data_census_income,500, \"logs/fit/census_income/500batches\",16)\n",
    "'''\n",
    "train_and_evaluate(data_communities_and_crime,500, \"logs/fit/communities_and_crime/500batches\",16)\n",
    "#train_and_evaluate(data_parkinsons_telemonitoring,500, \"logs/fit/parkinsons_telemonitoring/500batches\",16)\n",
    "#train_and_evaluate(data_phiusiil_phishing_url_website,500, \"logs/fit/phiusiil_phishing_url_website/500batches\",16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DgSXOSriJ2s"
   },
   "source": [
    "# **250 batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1740760719788,
     "user": {
      "displayName": "Victor",
      "userId": "05909548797419194511"
     },
     "user_tz": -60
    },
    "id": "wjiTWIiUiUFU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Create the model for 250 batches\\ntrain_and_evaluate(\\n    dataset=data_iris,\\n    num_batches = 250,\\n    log_dir=\"logs/fit/iris/250batches\",\\n    batch_size = 16\\n)\\ntrain_and_evaluate(\\n    dataset=data_heart_disease,\\n    num_batches = 250,\\n    log_dir=\"logs/fit/heart_disease/250batches\",\\n    batch_size = 16\\n)\\n\\ntrain_and_evaluate(\\n    dataset=data_breast_cancer_wisconsin_diagnostic,\\n    num_batches = 250,\\n    log_dir=\"logs/fit/breast_cancer_wisconsin_diagnostic/250batches\",\\n    batch_size = 16\\n)\\ntrain_and_evaluate(\\n    dataset=data_bank_marketing,\\n    num_batches = 250,\\n    log_dir=\"logs/fit/bank_marketing/250batches\",\\n    batch_size = 16\\n)\\ntrain_and_evaluate(\\n    dataset=data_diabetes,\\n    num_batches = 250,\\n    log_dir=\"logs/fit/diabetes/250batches\",\\n    batch_size = 16\\n)\\ntrain_and_evaluate(\\n    dataset=data_adult,\\n    num_batches = 250,\\n    log_dir=\"logs/fit/adult/250batches\",\\n    batch_size = 16\\n)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Create the model for 250 batches\n",
    "train_and_evaluate(\n",
    "    dataset=data_iris,\n",
    "    num_batches = 250,\n",
    "    log_dir=\"logs/fit/iris/250batches\",\n",
    "    batch_size = 16\n",
    ")\n",
    "train_and_evaluate(\n",
    "    dataset=data_heart_disease,\n",
    "    num_batches = 250,\n",
    "    log_dir=\"logs/fit/heart_disease/250batches\",\n",
    "    batch_size = 16\n",
    ")\n",
    "\n",
    "train_and_evaluate(\n",
    "    dataset=data_breast_cancer_wisconsin_diagnostic,\n",
    "    num_batches = 250,\n",
    "    log_dir=\"logs/fit/breast_cancer_wisconsin_diagnostic/250batches\",\n",
    "    batch_size = 16\n",
    ")\n",
    "train_and_evaluate(\n",
    "    dataset=data_bank_marketing,\n",
    "    num_batches = 250,\n",
    "    log_dir=\"logs/fit/bank_marketing/250batches\",\n",
    "    batch_size = 16\n",
    ")\n",
    "train_and_evaluate(\n",
    "    dataset=data_diabetes,\n",
    "    num_batches = 250,\n",
    "    log_dir=\"logs/fit/diabetes/250batches\",\n",
    "    batch_size = 16\n",
    ")\n",
    "train_and_evaluate(\n",
    "    dataset=data_adult,\n",
    "    num_batches = 250,\n",
    "    log_dir=\"logs/fit/adult/250batches\",\n",
    "    batch_size = 16\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvPt7axusuaB"
   },
   "source": [
    "# **100 batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "aborted",
     "timestamp": 1740760719789,
     "user": {
      "displayName": "Victor",
      "userId": "05909548797419194511"
     },
     "user_tz": -60
    },
    "id": "zcokjLAfs0TF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Create the model for 100 batches\\ntrain_and_evaluate(\\n    dataset=data_iris,\\n    num_batches = 100,\\n    log_dir=\"logs/fit/iris/100batches\",\\n    batch_size = 16\\n)\\ntrain_and_evaluate(\\n    dataset=data_heart_disease,\\n    num_batches = 100,\\n    log_dir=\"logs/fit/heart_disease/100batches\",\\n    batch_size = 16\\n)\\n\\ntrain_and_evaluate(\\n    dataset=data_breast_cancer_wisconsin_diagnostic,\\n    num_batches = 100,\\n    log_dir=\"logs/fit/breast_cancer_wisconsin_diagnostic/100batches\",\\n    batch_size = 16\\n)\\ntrain_and_evaluate(\\n    dataset=data_bank_marketing,\\n    num_batches = 100,\\n    log_dir=\"logs/fit/bank_marketing/100batches\",\\n    batch_size = 16\\n)\\ntrain_and_evaluate(\\n    dataset=data_diabetes,\\n    num_batches = 100,\\n    log_dir=\"logs/fit/diabetes/100batches\",\\n    batch_size = 16\\n)\\ntrain_and_evaluate(\\n    dataset=data_adult,\\n    num_batches = 100,\\n    log_dir=\"logs/fit/adult/100batches\",\\n    batch_size = 16\\n)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Create the model for 100 batches\n",
    "train_and_evaluate(\n",
    "    dataset=data_iris,\n",
    "    num_batches = 100,\n",
    "    log_dir=\"logs/fit/iris/100batches\",\n",
    "    batch_size = 16\n",
    ")\n",
    "train_and_evaluate(\n",
    "    dataset=data_heart_disease,\n",
    "    num_batches = 100,\n",
    "    log_dir=\"logs/fit/heart_disease/100batches\",\n",
    "    batch_size = 16\n",
    ")\n",
    "\n",
    "train_and_evaluate(\n",
    "    dataset=data_breast_cancer_wisconsin_diagnostic,\n",
    "    num_batches = 100,\n",
    "    log_dir=\"logs/fit/breast_cancer_wisconsin_diagnostic/100batches\",\n",
    "    batch_size = 16\n",
    ")\n",
    "train_and_evaluate(\n",
    "    dataset=data_bank_marketing,\n",
    "    num_batches = 100,\n",
    "    log_dir=\"logs/fit/bank_marketing/100batches\",\n",
    "    batch_size = 16\n",
    ")\n",
    "train_and_evaluate(\n",
    "    dataset=data_diabetes,\n",
    "    num_batches = 100,\n",
    "    log_dir=\"logs/fit/diabetes/100batches\",\n",
    "    batch_size = 16\n",
    ")\n",
    "train_and_evaluate(\n",
    "    dataset=data_adult,\n",
    "    num_batches = 100,\n",
    "    log_dir=\"logs/fit/adult/100batches\",\n",
    "    batch_size = 16\n",
    ")\n",
    "'''\n",
    "#problema en el numero de clases de salida (targets) de las neuronas en la perdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1740760719864,
     "user": {
      "displayName": "Victor",
      "userId": "05909548797419194511"
     },
     "user_tz": -60
    },
    "id": "gHE620fRNgdi"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-68398e060c894da\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-68398e060c894da\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
