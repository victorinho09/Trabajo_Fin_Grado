{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<a target=\"_blank\" href=\"https://colab.research.google.com/github/victorinho09/Trabajo_Fin_Grado/blob/main/TFGv2.ipynb\">\n","<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>"],"metadata":{"id":"p_JNb5Ado4lS"}},{"cell_type":"markdown","source":["First approach of a code base that loads a sample dataset and trains a basic neural network."],"metadata":{"id":"ORm3AGdSN9sT"}},{"cell_type":"code","source":["# EL problema estaba en el renombre del data_iris para conseguir las columnas,\n","#que las ponía a Nan, verificar que para el resto de datasets va bien\n","#Verificar numero de neuronas de salida para todos los datasets.\n","\n","#Comprobar que lo ultimo metido de chatgpt no da error ya\n","\n","\n","!pip install ucimlrepo\n","\n","#Imports\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.callbacks import TensorBoard\n","from sklearn.impute import SimpleImputer\n","\n","import datetime\n","from ucimlrepo import fetch_ucirepo"],"metadata":{"id":"K5GDpYD5P4DI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"97a9b3be-43b7-443d-893b-dff7bffe0e21","executionInfo":{"status":"ok","timestamp":1740760664894,"user_tz":-60,"elapsed":3032,"user":{"displayName":"Victor","userId":"05909548797419194511"}}},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.11/dist-packages (0.0.7)\n","Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2.2.2)\n","Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2025.1.31)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n"]}]},{"cell_type":"code","source":["data_iris = fetch_ucirepo(id=53).data #clasificacion\n","data_heart_disease = fetch_ucirepo(id=45).data #clasificacion\n","data_adult = fetch_ucirepo(id=2).data #clasificacion\n","data_breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17).data #clasificacion\n","data_bank_marketing = fetch_ucirepo(id=222).data #clasificacion\n","data_diabetes = fetch_ucirepo(id=296).data #clasificacion\n","data_mushroom = fetch_ucirepo(id=73).data #clasificacion\n","data_default_payment = fetch_ucirepo(id=350).data #clasificacion\n","data_car_evaluation = fetch_ucirepo(id=19).data #clasificacion\n","data_dry_bean = fetch_ucirepo(id=602).data #clasificacion\n","data_magic_gamma_telescope = fetch_ucirepo(id=159).data #clasificacion\n","data_spambase = fetch_ucirepo(id=94).data #clasificacion\n","data_census_income = fetch_ucirepo(id=20).data #clasificacion\n","#data_phiusiil_phishing_url_website = fetch_ucirepo(id=967).data #clasificacion. --> NO CABE EN RAM, DA ERROR\n","data_bike_sharing = fetch_ucirepo(id=275).data #regresion --> Este dataset funciona muy mal. ¿Mejorará con otra arquitectura?\n","#data_real_estate_valuation = fetch_ucirepo(id=477).data #regresion --> Problema: Demasiado numero de clases objetivo, y muy pocas instancias, por lo que no da suficiente para que stratify funcione en testset\n","data_communities_and_crime = fetch_ucirepo(id=183).data #regresion --> Funciona muy mal tmb, muchas clases objetivo\n","#data_parkinsons_telemonitoring = fetch_ucirepo(id=189).data #regresion -->No coge datos, no contiene nada\n"],"metadata":{"id":"sxDAjeQ9P-cv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9a631a14-3893-4053-d4ec-5f5dc353048c","executionInfo":{"status":"ok","timestamp":1740760714277,"user_tz":-60,"elapsed":49382,"user":{"displayName":"Victor","userId":"05909548797419194511"}}},"execution_count":83,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/ucimlrepo/fetch.py:97: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(data_url)\n"]}]},{"cell_type":"code","source":["\n","def get_y_target_col(data_obj):\n","    \"\"\"\n","    Dado data_obj = fetch_ucirepo(id=...).data, localiza la columna\n","    de 'targets' y la renombra a 'target'.\n","    Maneja varios casos:\n","     - Ya se llama 'target'\n","     - Se llama 'class'\n","     - Solo hay una columna en data.targets (la renombra a 'target')\n","    Devuelve un DataFrame con una sola columna llamada 'target'.\n","    \"\"\"\n","    df_targets = data_obj.targets\n","    cols = list(df_targets.columns)\n","\n","    # CASO 1: Si ya existe 'target'\n","    if 'target' in cols:\n","        y = df_targets[['target']].copy()\n","        print(\"Numero cols en y:\", y.columns)\n","        return y\n","\n","    # CASO 2: Si existe 'class'\n","    if 'class' in cols:\n","        y = df_targets[['class']].rename(columns={'class': 'target'})\n","        print(\"Numero cols en y:\", y.columns)\n","        return y\n","\n","    # CASO 3: Si solo hay 1 columna, la renombramos\n","    if len(cols) == 1:\n","        old_col = cols[0]\n","        y = df_targets.rename(columns={old_col: 'target'})\n","        return y[['target']]\n","\n","def preprocessData(data):\n","\n","    X = pd.DataFrame(data.features, columns=data.feature_names)\n","    y = get_y_target_col(data)\n","\n","    #Elimino las filas que contienen que pertenecen a una clase con menos de 2 instancias en todo el dataset,\n","    #porque sino stratify falla, al no poder balancear la aparición de clases en ambos sets al splittear\n","    counts = y['target'].value_counts()\n","    rare_classes = counts[counts < 2].index\n","    mask = ~y['target'].isin(rare_classes)\n","    X = X[mask].reset_index(drop=True)\n","    y = y[mask].reset_index(drop=True)\n","\n","    #Filtrar filas donde y['target'] sea NaN\n","    #(esto crea una máscara True/False, y solo conservas las True)\n","    mask_not_nan = ~y['target'].isna()\n","    X = X[mask_not_nan].reset_index(drop=True)\n","    y = y[mask_not_nan].reset_index(drop=True)\n","\n","    #Train/Test split\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=0.2, random_state=42, stratify=y #Hace falta stratify porque hay algún dataset (bike_sharing)\n","        #que contiene muchas clases objetivo y provoca que no esté balanceado el train y el test set.\n","    )\n","\n","    #print(y_train.info())\n","    print(\"y_train count values:\", y_train['target'].nunique())\n","    print(\"y_train values:\", y_train['target'].unique())\n","    #print(y_train.head(30))\n","\n","    '''\n","    print(\"X_train shape antes de imputer:\", X_train.shape)\n","    print(\"¿NaNs en X_train?\", X_train.isna().sum())\n","    '''\n","\n","    # 4) One-Hot Encoding para y\n","    encoder = OneHotEncoder(sparse_output=False)\n","    y_train_encoded = encoder.fit_transform(y_train['target'].values.reshape(-1, 1))\n","    y_test_encoded  = encoder.transform(y_test['target'].values.reshape(-1, 1))\n","    #print(\"y_test_encoded: \", y_test_encoded[1])\n","    #print(\"size_y_test_encoded \", len(y_train_encoded[1]))\n","\n","    # 5) Separar columnas numéricas y categóricas\n","    numeric_cols_train = X_train.select_dtypes(include=np.number).columns\n","    categorical_cols_train = X_train.select_dtypes(exclude=np.number).columns\n","\n","    if len(numeric_cols_train) == 0 and len(categorical_cols_train) == 0:\n","      raise ValueError(\"No hay columnas numéricas ni categóricas en X_train\")\n","\n","    numeric_cols_test = X_test.select_dtypes(include=np.number).columns\n","    categorical_cols_test = X_test.select_dtypes(exclude=np.number).columns\n","\n","    if len(numeric_cols_test) == 0 and len(categorical_cols_test) == 0:\n","      raise ValueError(\"No hay columnas numéricas ni categóricas en X_test\")\n","\n","    if(len(categorical_cols_train) == 0):\n","        print(\"NO hay columnas categoricas en X_train\")\n","\n","    # Imputar numéricas\n","    imputer_num = SimpleImputer(strategy='mean')\n","    if len(numeric_cols_train) > 0:\n","      X_train_num_imputed = imputer_num.fit_transform(X_train[numeric_cols_train])\n","      X_test_num_imputed  = imputer_num.transform(X_test[numeric_cols_test])\n","\n","      # Escalar numéricas\n","      scaler = StandardScaler()\n","      X_train_num_scaled = scaler.fit_transform(X_train_num_imputed)\n","      X_test_num_scaled  = scaler.transform(X_test_num_imputed)\n","\n","    else:\n","      X_train_num_scaled = np.zeros((X_train.shape[0], 0))\n","      X_test_num_scaled  = np.zeros((X_test.shape[0], 0))\n","\n","\n","    # Imputar categóricas\n","    imputer_cat = SimpleImputer(strategy='most_frequent')\n","    if len(categorical_cols_train) > 0:\n","      X_train_cat_imputed = imputer_cat.fit_transform(X_train[categorical_cols_train])\n","      X_test_cat_imputed  = imputer_cat.transform(X_test[categorical_cols_test])\n","    else:\n","      X_train_cat_imputed = np.zeros((X_train.shape[0], 0))\n","      X_test_cat_imputed  = np.zeros((X_test.shape[0], 0))\n","\n","    # Codificar categóricas\n","\n","    if(len(categorical_cols_train) != 0):\n","        print(\"Hay columnas categoricas en X_train\")\n","        ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n","        X_train_cat_encoded = ohe.fit_transform(X_train_cat_imputed)\n","        X_test_cat_encoded  = ohe.transform(X_test_cat_imputed)\n","\n","    else:\n","        X_train_cat_encoded = np.zeros((X_train.shape[0], 0))\n","        X_test_cat_encoded  = np.zeros((X_test.shape[0], 0))\n","\n","    # Concatenar numéricas escaladas + categóricas codificadas\n","    X_train_final = np.concatenate([X_train_num_scaled, X_train_cat_encoded], axis=1)\n","    X_test_final  = np.concatenate([X_test_num_scaled, X_test_cat_encoded],  axis=1)\n","\n","    # Revisar si hay NaN en la salida final\n","    print(\"¿Hay NaN en X_train_final?\", np.isnan(X_train_final).any())\n","\n","    return X_train_final, X_test_final, y_train_encoded, y_test_encoded\n","\n","#numero de neuronas rango (capa inicial) : raiz del numero de features - numero de features\n","#numero capas:\n","\n","\n"],"metadata":{"id":"lUfPdpdrbYVa","executionInfo":{"status":"ok","timestamp":1740760714280,"user_tz":-60,"elapsed":2,"user":{"displayName":"Victor","userId":"05909548797419194511"}}},"execution_count":84,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.callbacks import Callback\n","import math\n","\n","# Creation of the ANN of example to try out the batch metrics in TensorBoard\n","def create_model(X_train,y_train):\n","  print(\"y_train_shape: \",y_train.shape[1])\n","  model = Sequential([\n","      Input(shape=(X_train.shape[1],)),\n","      Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Hidden Layer 1\n","      Dense(32, activation='relu'),  # Hidden Layer 2\n","      Dense(y_train.shape[1], activation='softmax')  # Output Layer with softmax for multiclass clasification\n","  ])\n","\n","  # Compiling the model\n","  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #uso categorical_crossentropy cuando las etiquetas están codificadas con one-hot encoder. Si no usaría: sparse_categ_cross\n","\n","  return model\n","\n","class GlobalBatchLogger(Callback):\n","    def __init__(self, log_dir):\n","        super(GlobalBatchLogger, self).__init__()\n","        self.log_dir = log_dir\n","        self.global_step = 0\n","\n","        # Variables para acumulados\n","        self.cumulative_loss = 0.0\n","        self.cumulative_accuracy = 0.0\n","        self.total_samples = 0  # para saber cuántas muestras se han procesado\n","\n","    def on_train_begin(self, logs=None):\n","        # Creamos el escritor de resúmenes de TensorFlow al inicio del entrenamiento\n","        self.writer = tf.summary.create_file_writer(self.log_dir)\n","        self.writer.set_as_default()\n","\n","    def on_batch_end(self, batch, logs=None):\n","        \"\"\"\n","        Se llama al terminar cada batch.\n","        ‘batch’ es el índice local del batch en la época, pero usamos\n","        self.global_step para numerar de forma continua a lo largo\n","        de todo el entrenamiento.\n","        \"\"\"\n","        if logs is not None:\n","            # Obtenemos las métricas *promediadas* que Keras ha calculado hasta este batch\n","            batch_loss = logs.get('loss', 0.0)\n","            batch_acc = logs.get('accuracy', 0.0)\n","\n","            # Determinamos cuántas muestras se incluyeron en este batch\n","            batch_size = logs.get('size', None)\n","            if batch_size is None:\n","                # Si no aparece en logs, tomamos la configuración de la clase\n","                batch_size = self.params.get('batch_size', 1)\n","\n","            # Actualizamos la suma acumulada de (loss * nº muestras) y (acc * nº muestras)\n","            self.cumulative_loss += batch_loss * batch_size\n","            self.cumulative_accuracy += batch_acc * batch_size\n","            self.total_samples += batch_size\n","\n","            # Calculamos la media acumulada (hasta este batch)\n","            avg_cumulative_loss = self.cumulative_loss / self.total_samples\n","            avg_cumulative_accuracy = self.cumulative_accuracy / self.total_samples\n","\n","            # Registramos exclusivamente la métrica acumulada\n","            tf.summary.scalar('cumulative_loss', data=avg_cumulative_loss, step=self.global_step)\n","            tf.summary.scalar('cumulative_accuracy', data=avg_cumulative_accuracy, step=self.global_step)\n","\n","        # Forzamos la escritura en los ficheros de logs\n","        self.writer.flush()\n","\n","        # Incrementamos el contador global de batches\n","        self.global_step += 1\n","\n","    def on_train_end(self, logs=None):\n","        # Cerramos el writer al finalizar\n","        self.writer.close()\n","\n","def compute_steps_for_batches(\n","    desired_batches,\n","    X_train_scaled,\n","    batch_size=16\n","):\n","    \"\"\"\n","    Dado un número deseado de batches (desired_batches), calcula\n","    el número de steps que se pueden entrenar, sin pasarse\n","    del total de batches en X_train_scaled.\n","    - desired_batches: cuántos batches queremos.\n","    - X_train_scaled: datos de entrenamiento.\n","    - batch_size: tamaño de lote.\n","\n","    Retorna steps_for_n_batches, que es el mínimo entre desired_batches y\n","    la cantidad real de batches que hay.\n","    \"\"\"\n","    total_samples = X_train_scaled.shape[0]\n","    total_batches = math.ceil(total_samples / batch_size)\n","\n","    # Para no pasarnos de la época, usamos el mínimo\n","    steps_for_n_batches = min(desired_batches, total_batches)\n","\n","    # También puedes forzar que sea al menos 1\n","    if steps_for_n_batches < 1:\n","        steps_for_n_batches = 1\n","\n","    return steps_for_n_batches\n","\n","def train_and_evaluate(dataset,num_batches,log_dir,batch_size):\n","    X_train_scaled, X_test_scaled, y_train_encoded, y_test_encoded = preprocessData(dataset)\n","    model = create_model(X_train_scaled,y_train_encoded)\n","\n","    tb_callback = tf.keras.callbacks.TensorBoard(\n","        log_dir=log_dir,\n","        histogram_freq=1,\n","        update_freq='epoch'\n","    )\n","    global_batch_logger = GlobalBatchLogger(log_dir)\n","\n","\n","    steps_per_epoch = compute_steps_for_batches(num_batches,X_train_scaled,batch_size)\n","\n","    history = model.fit(\n","        X_train_scaled,\n","        y_train_encoded,\n","        validation_data=(X_test_scaled, y_test_encoded),\n","        epochs=1,\n","        steps_per_epoch = steps_per_epoch,\n","        batch_size=batch_size,\n","        verbose=0,\n","        callbacks=[tb_callback,global_batch_logger]\n","    )\n","    loss, accuracy = model.evaluate(X_test_scaled, y_test_encoded)\n","    print(f\"Steps_per_epoch: {steps_per_epoch},Pérdida: {loss:.4f}, Precisión: {accuracy:.4f}\")\n","    return history"],"metadata":{"id":"moHmIomtqnv_","executionInfo":{"status":"ok","timestamp":1740760714282,"user_tz":-60,"elapsed":1,"user":{"displayName":"Victor","userId":"05909548797419194511"}}},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":["# **500 batches**"],"metadata":{"id":"f5OTApYn9MI8"}},{"cell_type":"code","source":["'''\n","train_and_evaluate(data_iris,500, \"logs/fit/iris/500batches\",16)\n","train_and_evaluate(data_heart_disease,500, \"logs/fit/heart_disease/500batches\",16)\n","train_and_evaluate(data_breast_cancer_wisconsin_diagnostic,500, \"logs/fit/breast_cancer_wisconsin_diagnostic/500batches\",16)\n","train_and_evaluate(data_bank_marketing,500, \"logs/fit/bank_marketing/500batches\",16)\n","train_and_evaluate(data_diabetes,500, \"logs/fit/diabetes/500batches\",16)\n","train_and_evaluate(data_adult,500, \"logs/fit/adult/500batches\",16)\n","train_and_evaluate(data_mushroom,500, \"logs/fit/mushroom/500batches\",16)\n","train_and_evaluate(data_default_payment,500, \"logs/fit/default_payment/500batches\",16)\n","train_and_evaluate(data_bike_sharing,500, \"logs/fit/bike_sharing/500batches\",16)\n","train_and_evaluate(data_dry_bean,500, \"logs/fit/dry_bean/500batches\",16)\n","train_and_evaluate(data_spambase,500, \"logs/fit/spambase/500batches\",16)\n","train_and_evaluate(data_magic_gamma_telescope,500, \"logs/fit/magic_gamma_telescope/500batches\",16)\n","#train_and_evaluate(data_real_estate_valuation,500, \"logs/fit/real_estate_valuation/500batches\",16)\n","train_and_evaluate(data_car_evaluation,500, \"logs/fit/car_evaluation/500batches\",16)\n","train_and_evaluate(data_census_income,500, \"logs/fit/census_income/500batches\",16)\n","'''\n","train_and_evaluate(data_communities_and_crime,500, \"logs/fit/communities_and_crime/500batches\",16)\n","#train_and_evaluate(data_parkinsons_telemonitoring,500, \"logs/fit/parkinsons_telemonitoring/500batches\",16)\n","#train_and_evaluate(data_phiusiil_phishing_url_website,500, \"logs/fit/phiusiil_phishing_url_website/500batches\",16)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":636},"id":"OHOanZui9F8c","outputId":"8cdd3467-dfc5-48ce-8931-a664764aeaf0","executionInfo":{"status":"error","timestamp":1740760719761,"user_tz":-60,"elapsed":5478,"user":{"displayName":"Victor","userId":"05909548797419194511"}}},"execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":["y_train count values: 94\n","y_train values: [1.   0.01 0.56 0.11 0.06 0.36 0.18 0.44 0.05 0.1  0.04 0.22 0.15 0.03\n"," 0.02 0.16 0.07 0.57 0.2  0.09 0.14 0.25 0.48 0.41 0.08 0.29 0.19 0.12\n"," 0.54 0.   0.55 0.61 0.28 0.4  0.13 0.69 0.23 0.3  0.5  0.88 0.32 0.31\n"," 0.87 0.45 0.67 0.68 0.71 0.59 0.66 0.17 0.21 0.53 0.34 0.38 0.63 0.24\n"," 0.82 0.78 0.62 0.79 0.26 0.42 0.27 0.37 0.39 0.58 0.51 0.47 0.35 0.91\n"," 0.76 0.7  0.52 0.49 0.33 0.43 0.95 0.65 0.6  0.74 0.75 0.81 0.64 0.73\n"," 0.83 0.85 0.86 0.72 0.9  0.46 0.93 0.97 0.8  0.84]\n","Hay columnas categoricas en X_train\n","¿Hay NaN en X_train_final? False\n","y_train_shape:  94\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0683 - loss: 3.8501\n","Steps_per_epoch: 100,Pérdida: 3.8177, Precisión: 0.0653\n"]},{"output_type":"error","ename":"TypeError","evalue":"'NoneType' object is not subscriptable","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-86-f7bd45c7673e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m '''\n\u001b[1;32m     18\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_communities_and_crime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"logs/fit/communities_and_crime/500batches\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_parkinsons_telemonitoring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"logs/fit/parkinsons_telemonitoring/500batches\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#train_and_evaluate(data_phiusiil_phishing_url_website,500, \"logs/fit/phiusiil_phishing_url_website/500batches\",16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-85-8262a63e6024>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(dataset, num_batches, log_dir, batch_size)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-84-081d34d427c5>\u001b[0m in \u001b[0;36mpreprocessData\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#Elimino las filas que contienen que pertenecen a una clase con menos de 2 instancias en todo el dataset,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m#porque sino stratify falla, al no poder balancear la aparición de clases en ambos sets al splittear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mrare_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounts\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrare_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"]}]},{"cell_type":"markdown","source":["# **250 batches**"],"metadata":{"id":"8DgSXOSriJ2s"}},{"cell_type":"code","source":["'''\n","#Create the model for 250 batches\n","train_and_evaluate(\n","    dataset=data_iris,\n","    num_batches = 250,\n","    log_dir=\"logs/fit/iris/250batches\",\n","    batch_size = 16\n",")\n","train_and_evaluate(\n","    dataset=data_heart_disease,\n","    num_batches = 250,\n","    log_dir=\"logs/fit/heart_disease/250batches\",\n","    batch_size = 16\n",")\n","\n","train_and_evaluate(\n","    dataset=data_breast_cancer_wisconsin_diagnostic,\n","    num_batches = 250,\n","    log_dir=\"logs/fit/breast_cancer_wisconsin_diagnostic/250batches\",\n","    batch_size = 16\n",")\n","train_and_evaluate(\n","    dataset=data_bank_marketing,\n","    num_batches = 250,\n","    log_dir=\"logs/fit/bank_marketing/250batches\",\n","    batch_size = 16\n",")\n","train_and_evaluate(\n","    dataset=data_diabetes,\n","    num_batches = 250,\n","    log_dir=\"logs/fit/diabetes/250batches\",\n","    batch_size = 16\n",")\n","train_and_evaluate(\n","    dataset=data_adult,\n","    num_batches = 250,\n","    log_dir=\"logs/fit/adult/250batches\",\n","    batch_size = 16\n",")\n","'''"],"metadata":{"id":"wjiTWIiUiUFU","executionInfo":{"status":"aborted","timestamp":1740760719788,"user_tz":-60,"elapsed":1,"user":{"displayName":"Victor","userId":"05909548797419194511"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **100 batches**"],"metadata":{"id":"hvPt7axusuaB"}},{"cell_type":"code","source":["'''\n","#Create the model for 100 batches\n","train_and_evaluate(\n","    dataset=data_iris,\n","    num_batches = 100,\n","    log_dir=\"logs/fit/iris/100batches\",\n","    batch_size = 16\n",")\n","train_and_evaluate(\n","    dataset=data_heart_disease,\n","    num_batches = 100,\n","    log_dir=\"logs/fit/heart_disease/100batches\",\n","    batch_size = 16\n",")\n","\n","train_and_evaluate(\n","    dataset=data_breast_cancer_wisconsin_diagnostic,\n","    num_batches = 100,\n","    log_dir=\"logs/fit/breast_cancer_wisconsin_diagnostic/100batches\",\n","    batch_size = 16\n",")\n","train_and_evaluate(\n","    dataset=data_bank_marketing,\n","    num_batches = 100,\n","    log_dir=\"logs/fit/bank_marketing/100batches\",\n","    batch_size = 16\n",")\n","train_and_evaluate(\n","    dataset=data_diabetes,\n","    num_batches = 100,\n","    log_dir=\"logs/fit/diabetes/100batches\",\n","    batch_size = 16\n",")\n","train_and_evaluate(\n","    dataset=data_adult,\n","    num_batches = 100,\n","    log_dir=\"logs/fit/adult/100batches\",\n","    batch_size = 16\n",")\n","'''\n","#problema en el numero de clases de salida (targets) de las neuronas en la perdida"],"metadata":{"id":"zcokjLAfs0TF","executionInfo":{"status":"aborted","timestamp":1740760719789,"user_tz":-60,"elapsed":0,"user":{"displayName":"Victor","userId":"05909548797419194511"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logdir logs/fit"],"metadata":{"id":"gHE620fRNgdi","executionInfo":{"status":"aborted","timestamp":1740760719864,"user_tz":-60,"elapsed":3,"user":{"displayName":"Victor","userId":"05909548797419194511"}}},"execution_count":null,"outputs":[]}]}