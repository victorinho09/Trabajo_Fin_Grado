{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<a target=\"_blank\" href=\"https://colab.research.google.com/github/victorinho09/Trabajo_Fin_Grado/blob/main/TFGv2.ipynb\">\n","<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>"],"metadata":{"id":"p_JNb5Ado4lS"}},{"cell_type":"markdown","source":["First approach of a code base that loads a sample dataset and trains a basic neural network."],"metadata":{"id":"ORm3AGdSN9sT"}},{"cell_type":"code","source":["# EL problema estaba en el renombre del data_iris para conseguir las columnas,\n","#que las ponía a Nan, verificar que para el resto de datasets va bien\n","#Verificar numero de neuronas de salida para todos los datasets.\n","\n","#Comprobar que lo ultimo metido de chatgpt no da error ya\n","\n","\n","!pip install ucimlrepo\n","\n","#Imports\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.callbacks import TensorBoard\n","from sklearn.impute import SimpleImputer\n","\n","import datetime\n","from ucimlrepo import fetch_ucirepo"],"metadata":{"id":"K5GDpYD5P4DI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"877b5813-48ef-40bd-fca5-a420877756e5","executionInfo":{"status":"ok","timestamp":1740218700775,"user_tz":-60,"elapsed":4944,"user":{"displayName":"Victor","userId":"05909548797419194511"}}},"execution_count":237,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.11/dist-packages (0.0.7)\n","Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2.2.2)\n","Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.11/dist-packages (from ucimlrepo) (2025.1.31)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n"]}]},{"cell_type":"code","source":["data_iris = fetch_ucirepo(id=53).data\n","data_heart_disease = fetch_ucirepo(id=45).data\n","data_adult = fetch_ucirepo(id=2).data\n","data_breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17).data\n","data_bank_marketing = fetch_ucirepo(id=222).data\n","data_diabetes = fetch_ucirepo(id=296).data"],"metadata":{"id":"sxDAjeQ9P-cv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c71862c2-d477-4a4c-c3cb-75fa5f370958","executionInfo":{"status":"ok","timestamp":1740218706776,"user_tz":-60,"elapsed":6002,"user":{"displayName":"Victor","userId":"05909548797419194511"}}},"execution_count":238,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/ucimlrepo/fetch.py:97: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(data_url)\n"]}]},{"cell_type":"code","source":["\n","def get_y_target_col(data_obj):\n","    \"\"\"\n","    Dado data_obj = fetch_ucirepo(id=...).data, localiza la columna\n","    de 'targets' y la renombra a 'target'.\n","    Maneja varios casos:\n","     - Ya se llama 'target'\n","     - Se llama 'class'\n","     - Solo hay una columna en data.targets (la renombra a 'target')\n","    Devuelve un DataFrame con una sola columna llamada 'target'.\n","    \"\"\"\n","    df_targets = data_obj.targets\n","    cols = list(df_targets.columns)\n","\n","    # CASO 1: Si ya existe 'target'\n","    if 'target' in cols:\n","        y = df_targets[['target']].copy()\n","        print(\"Numero cols en y:\", y.columns)\n","        return y\n","\n","    # CASO 2: Si existe 'class'\n","    if 'class' in cols:\n","        y = df_targets[['class']].rename(columns={'class': 'target'})\n","        print(\"Numero cols en y:\", y.columns)\n","        return y\n","\n","    # CASO 3: Si solo hay 1 columna, la renombramos\n","    if len(cols) == 1:\n","        old_col = cols[0]\n","        y = df_targets.rename(columns={old_col: 'target'})\n","        return y[['target']]\n","\n","def preprocessData(data):\n","\n","    X = pd.DataFrame(data.features, columns=data.feature_names)\n","    y = get_y_target_col(data)\n","\n","    # 2) Filtrar filas donde y['target'] sea NaN\n","    #    (esto crea una máscara True/False, y solo conservas las True)\n","    mask_not_nan = ~y['target'].isna()\n","    X = X[mask_not_nan].reset_index(drop=True)\n","    y = y[mask_not_nan].reset_index(drop=True)\n","\n","    # 1) Train/Test split\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=0.2, random_state=42\n","    )\n","\n","    #print(y_train.info())\n","    print(\"y_train count values:\", y_train['target'].nunique())\n","    print(\"y_train values:\", y_train['target'].unique())\n","    #print(y_train.head(30))\n","\n","    '''\n","    print(\"X_train shape antes de imputer:\", X_train.shape)\n","    print(\"¿NaNs en X_train?\", X_train.isna().sum())\n","    '''\n","\n","    # 4) One-Hot Encoding para y\n","    encoder = OneHotEncoder(sparse_output=False)\n","    y_train_encoded = encoder.fit_transform(y_train['target'].values.reshape(-1, 1))\n","    y_test_encoded  = encoder.transform(y_test['target'].values.reshape(-1, 1))\n","    #print(\"y_test_encoded: \", y_test_encoded[1])\n","    #print(\"size_y_test_encoded \", len(y_train_encoded[1]))\n","\n","    # 5) Separar columnas numéricas y categóricas\n","    numeric_cols_train = X_train.select_dtypes(include=np.number).columns\n","    categorical_cols_train = X_train.select_dtypes(exclude=np.number).columns\n","\n","    if len(numeric_cols_train) == 0 and len(categorical_cols_train) == 0:\n","      raise ValueError(\"No hay columnas numéricas ni categóricas en X_train\")\n","\n","    numeric_cols_test = X_test.select_dtypes(include=np.number).columns\n","    categorical_cols_test = X_test.select_dtypes(exclude=np.number).columns\n","\n","    if len(numeric_cols_test) == 0 and len(categorical_cols_test) == 0:\n","      raise ValueError(\"No hay columnas numéricas ni categóricas en X_test\")\n","\n","    if(len(categorical_cols_train) == 0):\n","        print(\"NO hay columnas categoricas en X_train\")\n","\n","    # **Paso A**: Imputar numéricas\n","    imputer_num = SimpleImputer(strategy='mean')\n","    if len(numeric_cols_train) > 0:\n","      X_train_num_imputed = imputer_num.fit_transform(X_train[numeric_cols_train])\n","      X_test_num_imputed  = imputer_num.transform(X_test[numeric_cols_test])\n","    else:\n","      X_train_num = np.zeros((X_train.shape[0], 0))\n","      X_test_num  = np.zeros((X_test.shape[0], 0))\n","\n","    # **Paso B**: Escalar numéricas\n","    scaler = StandardScaler()\n","    X_train_num_scaled = scaler.fit_transform(X_train_num_imputed)\n","    X_test_num_scaled  = scaler.transform(X_test_num_imputed)\n","\n","    # **Paso C**: Imputar categóricas\n","    imputer_cat = SimpleImputer(strategy='most_frequent')\n","    if len(categorical_cols_train) > 0:\n","      X_train_cat_imputed = imputer_cat.fit_transform(X_train[categorical_cols_train])\n","      X_test_cat_imputed  = imputer_cat.transform(X_test[categorical_cols_test])\n","    else:\n","      X_train_cat_imputed = np.zeros((X_train.shape[0], 0))\n","      X_test_cat_imputed  = np.zeros((X_test.shape[0], 0))\n","\n","    # **Paso D**: Codificar categóricas\n","\n","    if(len(categorical_cols_train) != 0):\n","        print(\"Hay columnas categoricas en X_train\")\n","        ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n","        X_train_cat_encoded = ohe.fit_transform(X_train_cat_imputed)\n","        X_test_cat_encoded  = ohe.transform(X_test_cat_imputed)\n","\n","    else:\n","        X_train_cat_encoded = np.zeros((X_train.shape[0], 0))\n","        X_test_cat_encoded  = np.zeros((X_test.shape[0], 0))\n","\n","    # **Paso E**: Concatenar numéricas escaladas + categóricas codificadas\n","    X_train_final = np.concatenate([X_train_num_scaled, X_train_cat_encoded], axis=1)\n","    X_test_final  = np.concatenate([X_test_num_scaled, X_test_cat_encoded],  axis=1)\n","\n","    # Revisar si hay NaN en la salida final\n","    print(\"¿Hay NaN en X_train_final?\", np.isnan(X_train_final).any())\n","\n","    return X_train_final, X_test_final, y_train_encoded, y_test_encoded\n","\n","#numero de neuronas rango (capa inicial) : raiz del numero de features - numero de features\n","#numero capas:\n","\n","\n"],"metadata":{"id":"lUfPdpdrbYVa","executionInfo":{"status":"ok","timestamp":1740218706776,"user_tz":-60,"elapsed":3,"user":{"displayName":"Victor","userId":"05909548797419194511"}}},"execution_count":239,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.callbacks import Callback\n","import math\n","\n","# Creation of the ANN of example to try out the batch metrics in TensorBoard\n","def create_model(X_train,y_train,y_test):\n","  print(\"y_train_shape: \",y_train.shape[1])\n","  model = Sequential([\n","      Input(shape=(X_train.shape[1],)),\n","      Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Hidden Layer 1\n","      Dense(32, activation='relu'),  # Hidden Layer 2\n","      Dense(y_train.shape[1], activation='softmax')  # Output Layer with softmax for multiclass clasification\n","  ])\n","\n","  # Compiling the model\n","  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #uso categorical_crossentropy cuando las etiquetas están codificadas con one-hot encoder. Si no usaría: sparse_categ_cross\n","\n","  return model\n","\n","class GlobalBatchLogger(Callback):\n","    def __init__(self, log_dir):\n","        super(GlobalBatchLogger, self).__init__()\n","        self.log_dir = log_dir\n","        self.global_step = 0\n","\n","        # Variables para acumulados\n","        self.cumulative_loss = 0.0\n","        self.cumulative_accuracy = 0.0\n","        self.total_samples = 0  # para saber cuántas muestras se han procesado\n","\n","    def on_train_begin(self, logs=None):\n","        # Creamos el escritor de resúmenes de TensorFlow al inicio del entrenamiento\n","        self.writer = tf.summary.create_file_writer(self.log_dir)\n","        self.writer.set_as_default()\n","\n","    def on_batch_end(self, batch, logs=None):\n","        \"\"\"\n","        Se llama al terminar cada batch.\n","        ‘batch’ es el índice local del batch en la época, pero usamos\n","        self.global_step para numerar de forma continua a lo largo\n","        de todo el entrenamiento.\n","        \"\"\"\n","        if logs is not None:\n","            # Obtenemos las métricas *promediadas* que Keras ha calculado hasta este batch\n","            batch_loss = logs.get('loss', 0.0)\n","            batch_acc = logs.get('accuracy', 0.0)\n","\n","            # Determinamos cuántas muestras se incluyeron en este batch\n","            batch_size = logs.get('size', None)\n","            if batch_size is None:\n","                # Si no aparece en logs, tomamos la configuración de la clase\n","                batch_size = self.params.get('batch_size', 1)\n","\n","            # Actualizamos la suma acumulada de (loss * nº muestras) y (acc * nº muestras)\n","            self.cumulative_loss += batch_loss * batch_size\n","            self.cumulative_accuracy += batch_acc * batch_size\n","            self.total_samples += batch_size\n","\n","            # Calculamos la media acumulada (hasta este batch)\n","            avg_cumulative_loss = self.cumulative_loss / self.total_samples\n","            avg_cumulative_accuracy = self.cumulative_accuracy / self.total_samples\n","\n","            # Registramos exclusivamente la métrica acumulada\n","            tf.summary.scalar('cumulative_loss', data=avg_cumulative_loss, step=self.global_step)\n","            tf.summary.scalar('cumulative_accuracy', data=avg_cumulative_accuracy, step=self.global_step)\n","\n","        # Forzamos la escritura en los ficheros de logs\n","        self.writer.flush()\n","\n","        # Incrementamos el contador global de batches\n","        self.global_step += 1\n","\n","    def on_train_end(self, logs=None):\n","        # Cerramos el writer al finalizar\n","        self.writer.close()\n","\n","def compute_steps_for_batches(\n","    desired_batches,\n","    X_train_scaled,\n","    batch_size=16\n","):\n","    \"\"\"\n","    Dado un número deseado de batches (desired_batches), calcula\n","    el número de steps que se pueden entrenar, sin pasarse\n","    del total de batches en X_train_scaled.\n","    - desired_batches: cuántos batches queremos.\n","    - X_train_scaled: datos de entrenamiento.\n","    - batch_size: tamaño de lote.\n","\n","    Retorna steps_for_n_batches, que es el mínimo entre desired_batches y\n","    la cantidad real de batches que hay.\n","    \"\"\"\n","    total_samples = X_train_scaled.shape[0]\n","    total_batches = math.ceil(total_samples / batch_size)\n","\n","    # Para no pasarnos de la época, usamos el mínimo\n","    steps_for_n_batches = min(desired_batches, total_batches)\n","\n","    # También puedes forzar que sea al menos 1\n","    if steps_for_n_batches < 1:\n","        steps_for_n_batches = 1\n","\n","    return steps_for_n_batches\n","\n","def train_and_evaluate(dataset,num_batches,log_dir,batch_size):\n","    X_train_scaled, X_test_scaled, y_train_encoded, y_test_encoded = preprocessData(dataset)\n","    model = create_model(X_train_scaled,y_train_encoded,y_test_encoded)\n","\n","    tb_callback = tf.keras.callbacks.TensorBoard(\n","        log_dir=log_dir,\n","        histogram_freq=1,\n","        update_freq='epoch'\n","    )\n","    global_batch_logger = GlobalBatchLogger(log_dir)\n","\n","\n","    steps_per_epoch = compute_steps_for_batches(num_batches,X_train_scaled,batch_size)\n","\n","    history = model.fit(\n","        X_train_scaled,\n","        y_train_encoded,\n","        validation_data=(X_test_scaled, y_test_encoded),\n","        epochs=1,\n","        steps_per_epoch = steps_per_epoch,\n","        batch_size=batch_size,\n","        verbose=0,\n","        callbacks=[tb_callback,global_batch_logger]\n","    )\n","    loss, accuracy = model.evaluate(X_test_scaled, y_test_encoded)\n","    print(f\"Steps_per_epoch: {steps_per_epoch},Pérdida: {loss:.4f}, Precisión: {accuracy:.4f}\")\n","    return history"],"metadata":{"id":"moHmIomtqnv_","executionInfo":{"status":"ok","timestamp":1740218706776,"user_tz":-60,"elapsed":2,"user":{"displayName":"Victor","userId":"05909548797419194511"}}},"execution_count":240,"outputs":[]},{"cell_type":"markdown","source":["# **500 batches**"],"metadata":{"id":"f5OTApYn9MI8"}},{"cell_type":"code","source":["train_and_evaluate(data_iris,500, \"logs/fit/iris/500batches\",16)\n","train_and_evaluate(data_heart_disease,500, \"logs/fit/heart_disease/500batches\",16)\n","\n","train_and_evaluate(data_breast_cancer_wisconsin_diagnostic,500, \"logs/fit/breast_cancer_wisconsin_diagnostic/500batches\",16)\n","train_and_evaluate(data_bank_marketing,500, \"logs/fit/bank_marketing/500batches\",16)\n","train_and_evaluate(data_diabetes,500, \"logs/fit/diabetes/500batches\",16)\n","train_and_evaluate(data_adult,500, \"logs/fit/adult/500batches\",16)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OHOanZui9F8c","outputId":"1c9fe0e4-aaff-46ed-a318-1bb0a8efb2c7","executionInfo":{"status":"ok","timestamp":1740218720980,"user_tz":-60,"elapsed":14206,"user":{"displayName":"Victor","userId":"05909548797419194511"}}},"execution_count":241,"outputs":[{"output_type":"stream","name":"stdout","text":["y_train count values: 4\n","y_train values: ['<=50K.' '>50K' '<=50K' '>50K.']\n","Hay columnas categoricas en X_train\n","¿Hay NaN en X_train_final? False\n","y_train_shape:  4\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5624 - loss: 0.9586\n","Steps_per_epoch: 500,Pérdida: 0.9590, Precisión: 0.5626\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7f414ae62750>"]},"metadata":{},"execution_count":241}]},{"cell_type":"markdown","source":["# **250 batches**"],"metadata":{"id":"8DgSXOSriJ2s"}},{"cell_type":"code","source":["\n","#Create the model for 250 batches\n","train_and_evaluate(\n","    dataset=data_iris,\n","    num_batches = 250,\n","    log_dir=\"logs/fit/iris/250batches\",\n","    batch_size = 16\n",")\n","'''\n","train_and_evaluate(\n","    dataset=data_heart_disease,\n","    num_batches = 250,\n","    log_dir=\"logs/fit/heart_disease/250batches\",\n","    batch_size = 16\n",")\n","\n","train_and_evaluate(\n","    dataset=data_breast_cancer_wisconsin_diagnostic,\n","    num_batches = 250,\n","    log_dir=\"logs/fit/breast_cancer_wisconsin_diagnostic/250batches\",\n","    batch_size = 16\n",")\n","train_and_evaluate(\n","    dataset=data_bank_marketing,\n","    num_batches = 250,\n","    log_dir=\"logs/fit/bank_marketing/250batches\",\n","    batch_size = 16\n",")\n","train_and_evaluate(\n","    dataset=data_diabetes,\n","    num_batches = 250,\n","    log_dir=\"logs/fit/diabetes/250batches\",\n","    batch_size = 16\n",")\n","train_and_evaluate(\n","    dataset=data_adult,\n","    num_batches = 250,\n","    log_dir=\"logs/fit/adult/250batches\",\n","    batch_size = 16\n",")\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333},"id":"wjiTWIiUiUFU","outputId":"c4aed82c-2223-4dac-8d8a-35b059fd9a6b","executionInfo":{"status":"ok","timestamp":1740218728190,"user_tz":-60,"elapsed":7213,"user":{"displayName":"Victor","userId":"05909548797419194511"}}},"execution_count":242,"outputs":[{"output_type":"stream","name":"stdout","text":["Numero cols en y: Index(['target'], dtype='object')\n","y_train count values: 3\n","y_train values: ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n","NO hay columnas categoricas en X_train\n","¿Hay NaN en X_train_final? False\n","y_train_shape:  3\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.6333 - loss: 0.9767\n","Steps_per_epoch: 8,Pérdida: 0.9767, Precisión: 0.6333\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\ntrain_and_evaluate(\\n    dataset=data_heart_disease,\\n    num_batches = 250,\\n    log_dir=\"logs/fit/heart_disease/250batches\",\\n    batch_size = 16\\n)\\n\\ntrain_and_evaluate(\\n    dataset=data_breast_cancer_wisconsin_diagnostic,\\n    num_batches = 250,\\n    log_dir=\"logs/fit/breast_cancer_wisconsin_diagnostic/250batches\",\\n    batch_size = 16\\n)\\ntrain_and_evaluate(\\n    dataset=data_bank_marketing,\\n    num_batches = 250,\\n    log_dir=\"logs/fit/bank_marketing/250batches\",\\n    batch_size = 16\\n)\\ntrain_and_evaluate(\\n    dataset=data_diabetes,\\n    num_batches = 250,\\n    log_dir=\"logs/fit/diabetes/250batches\",\\n    batch_size = 16\\n)\\ntrain_and_evaluate(\\n    dataset=data_adult,\\n    num_batches = 250,\\n    log_dir=\"logs/fit/adult/250batches\",\\n    batch_size = 16\\n)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":242}]},{"cell_type":"markdown","source":["# **100 batches**"],"metadata":{"id":"hvPt7axusuaB"}},{"cell_type":"code","source":["\n","#Create the model for 100 batches\n","train_and_evaluate(\n","    dataset=data_iris,\n","    num_batches = 100,\n","    log_dir=\"logs/fit/iris/100batches\",\n","    batch_size = 16\n",")\n","'''\n","train_and_evaluate(\n","    dataset=data_heart_disease,\n","    num_batches = 100,\n","    log_dir=\"logs/fit/heart_disease/100batches\",\n","    batch_size = 16\n",")\n","\n","train_and_evaluate(\n","    dataset=data_breast_cancer_wisconsin_diagnostic,\n","    num_batches = 100,\n","    log_dir=\"logs/fit/breast_cancer_wisconsin_diagnostic/100batches\",\n","    batch_size = 16\n",")\n","train_and_evaluate(\n","    dataset=data_bank_marketing,\n","    num_batches = 100,\n","    log_dir=\"logs/fit/bank_marketing/100batches\",\n","    batch_size = 16\n",")\n","train_and_evaluate(\n","    dataset=data_diabetes,\n","    num_batches = 100,\n","    log_dir=\"logs/fit/diabetes/100batches\",\n","    batch_size = 16\n",")\n","train_and_evaluate(\n","    dataset=data_adult,\n","    num_batches = 100,\n","    log_dir=\"logs/fit/adult/100batches\",\n","    batch_size = 16\n",")\n","\n","#problema en el numero de clases de salida (targets) de las neuronas en la perdida\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"id":"zcokjLAfs0TF","outputId":"0eac4300-72cf-4e66-df79-c3e5be401739","executionInfo":{"status":"ok","timestamp":1740218738433,"user_tz":-60,"elapsed":10251,"user":{"displayName":"Victor","userId":"05909548797419194511"}}},"execution_count":243,"outputs":[{"output_type":"stream","name":"stdout","text":["Numero cols en y: Index(['target'], dtype='object')\n","y_train count values: 3\n","y_train values: ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n","NO hay columnas categoricas en X_train\n","¿Hay NaN en X_train_final? False\n","y_train_shape:  3\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8333 - loss: 0.9647\n","Steps_per_epoch: 8,Pérdida: 0.9647, Precisión: 0.8333\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\ntrain_and_evaluate(\\n    dataset=data_heart_disease,\\n    num_batches = 100,\\n    log_dir=\"logs/fit/heart_disease/100batches\",\\n    batch_size = 16\\n)\\n\\ntrain_and_evaluate(\\n    dataset=data_breast_cancer_wisconsin_diagnostic,\\n    num_batches = 100,\\n    log_dir=\"logs/fit/breast_cancer_wisconsin_diagnostic/100batches\",\\n    batch_size = 16\\n)\\ntrain_and_evaluate(\\n    dataset=data_bank_marketing,\\n    num_batches = 100,\\n    log_dir=\"logs/fit/bank_marketing/100batches\",\\n    batch_size = 16\\n)\\ntrain_and_evaluate(\\n    dataset=data_diabetes,\\n    num_batches = 100,\\n    log_dir=\"logs/fit/diabetes/100batches\",\\n    batch_size = 16\\n)\\ntrain_and_evaluate(\\n    dataset=data_adult,\\n    num_batches = 100,\\n    log_dir=\"logs/fit/adult/100batches\",\\n    batch_size = 16\\n)\\n\\n#problema en el numero de clases de salida (targets) de las neuronas en la perdida\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":243}]},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logdir logs/fit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"gHE620fRNgdi","outputId":"e5269128-c6ae-4836-ba83-aa5c47de0d59","executionInfo":{"status":"ok","timestamp":1740218739146,"user_tz":-60,"elapsed":8,"user":{"displayName":"Victor","userId":"05909548797419194511"}}},"execution_count":244,"outputs":[{"output_type":"stream","name":"stdout","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]},{"output_type":"display_data","data":{"text/plain":["Reusing TensorBoard on port 6006 (pid 4460), started 1:50:04 ago. (Use '!kill 4460' to kill it.)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        (async () => {\n","            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n","            url.searchParams.set('tensorboardColab', 'true');\n","            const iframe = document.createElement('iframe');\n","            iframe.src = url;\n","            iframe.setAttribute('width', '100%');\n","            iframe.setAttribute('height', '800');\n","            iframe.setAttribute('frameborder', 0);\n","            document.body.appendChild(iframe);\n","        })();\n","    "]},"metadata":{}}]}]}